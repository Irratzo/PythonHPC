{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Numba for GPUs\n",
    "\n",
    "*Vasileios Karakasis, **Theofilos Manitaras**, CSCS*\n",
    "\n",
    "You may use Numba to generate GPU code from high-level Python functions. Both CUDA and ROCm GPUs (NVIDIA and AMD, respectively) are supported, but we are going to focus only on the CUDA capabilities of Numba.\n",
    "\n",
    "There is an almost one-to-one mapping between the Numba high-level Python abstractions and the different CUDA constructs. This practically means that, as a programmer, you need to take care explicitly of the host/device memory management and you need to be aware of the CUDA programming model.\n",
    "\n",
    "This demo will teach you to write your first CUDA kernels using Numba. It will cover the basic CUDA programming principles, but it should be enough to kick start you in GPU programming. More specifically, we will cover the following topics:\n",
    "\n",
    "- Writing a GPU kernel\n",
    "- Moving data to/from the GPU.\n",
    "- Spawning a GPU kernel\n",
    "- Profiling a GPU kernel\n",
    "- Optimizing memory accesses\n",
    "- Making use of the shared memory\n",
    "\n",
    "We will not cover CUDA streams.\n",
    "\n",
    "## Verify that Numba sees the GPU and understands CUDA\n",
    "\n",
    "First thing is to check if Numba can detect the GPU. You can achieve this by running the `numba` executable that comes with Numba's installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System info:\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m--------------------------------------------------------------------------------\n",
      "__Time Stamp__\n",
      "Report started (local time)                   : 2022-06-20 13:52:55.653965\n",
      "UTC start time                                : 2022-06-20 11:52:55.653969\n",
      "Running time (s)                              : 1.036071\n",
      "\n",
      "__Hardware Information__\n",
      "Machine                                       : x86_64\n",
      "CPU Name                                      : skylake\n",
      "CPU Count                                     : 12\n",
      "Number of accessible CPUs                     : 12\n",
      "List of accessible CPUs cores                 : 0 1 2 3 4 5 6 7 8 9 10 11\n",
      "CFS Restrictions (CPUs worth of runtime)      : None\n",
      "\n",
      "CPU Features                                  : 64bit adx aes avx avx2 bmi bmi2\n",
      "                                                clflushopt cmov cx16 cx8 f16c fma\n",
      "                                                fsgsbase fxsr invpcid lzcnt mmx\n",
      "                                                movbe pclmul popcnt prfchw rdrnd\n",
      "                                                rdseed rtm sahf sgx sse sse2 sse3\n",
      "                                                sse4.1 sse4.2 ssse3 xsave xsavec\n",
      "                                                xsaveopt xsaves\n",
      "\n",
      "Memory Total (MB)                             : 15974\n",
      "Memory Available (MB)                         : 11588\n",
      "\n",
      "__OS Information__\n",
      "Platform Name                                 : Linux-4.15.0-180-generic-x86_64-with-glibc2.27\n",
      "Platform Release                              : 4.15.0-180-generic\n",
      "OS Name                                       : Linux\n",
      "OS Version                                    : #189-Ubuntu SMP Wed May 18 14:13:57 UTC 2022\n",
      "OS Specific Version                           : ?\n",
      "Libc Version                                  : glibc 2.27\n",
      "\n",
      "__Python Information__\n",
      "Python Compiler                               : GCC 5.4.0 20160609\n",
      "Python Implementation                         : CPython\n",
      "Python Version                                : 3.9.0\n",
      "Python Locale                                 : en_US.UTF-8\n",
      "\n",
      "__Numba Toolchain Versions__\n",
      "Numba Version                                 : 0.55.2\n",
      "llvmlite Version                              : 0.38.1\n",
      "\n",
      "__LLVM Information__\n",
      "LLVM Version                                  : 11.1.0\n",
      "\n",
      "__CUDA Information__\n",
      "CUDA Device Initialized                       : True\n",
      "CUDA Driver Version                           : (11, 4)\n",
      "CUDA Runtime Version                          : 11040\n",
      "CUDA NVIDIA Bindings Available                : False\n",
      "CUDA NVIDIA Bindings In Use                   : False\n",
      "CUDA Detect Output:\n",
      "Found 1 CUDA devices\n",
      "id 0    b'NVIDIA GeForce GTX 970'                 [SUPPORTED (DEPRECATED)]\n",
      "                      Compute Capability: 5.2\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 1\n",
      "                                    UUID: GPU-0e831d63-ca54-b7b2-18d4-f610c95658b0\n",
      "                                Watchdog: Enabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "Summary:\n",
      "\t1/1 devices are supported\n",
      "\n",
      "CUDA Libraries Test Output:\n",
      "Finding nvvm from System\n",
      "\tnamed  libnvvm.so.4.0.0\n",
      "\ttrying to open library...\tok\n",
      "Finding cudart from System\n",
      "\tnamed  libcudart.so.11.4.148\n",
      "\ttrying to open library...\tok\n",
      "Finding cudadevrt from System\n",
      "\tnamed  libcudadevrt.a\n",
      "Finding libdevice from System\n",
      "\tsearching for compute_20...\tok\n",
      "\tsearching for compute_30...\tok\n",
      "\tsearching for compute_35...\tok\n",
      "\tsearching for compute_50...\tok\n",
      "\n",
      "\n",
      "__NumPy Information__\n",
      "NumPy Version                                 : 1.22.4\n",
      "NumPy Supported SIMD features                 : ('MMX', 'SSE', 'SSE2', 'SSE3', 'SSSE3', 'SSE41', 'POPCNT', 'SSE42', 'AVX', 'F16C', 'FMA3', 'AVX2')\n",
      "NumPy Supported SIMD dispatch                 : ('SSSE3', 'SSE41', 'POPCNT', 'SSE42', 'AVX', 'F16C', 'FMA3', 'AVX2', 'AVX512F', 'AVX512CD', 'AVX512_KNL', 'AVX512_KNM', 'AVX512_SKX', 'AVX512_CLX', 'AVX512_CNL', 'AVX512_ICL')\n",
      "NumPy Supported SIMD baseline                 : ('SSE', 'SSE2', 'SSE3')\n",
      "NumPy AVX512_SKX support detected             : False\n",
      "\n",
      "__SVML Information__\n",
      "SVML State, config.USING_SVML                 : False\n",
      "SVML Library Loaded                           : False\n",
      "llvmlite Using SVML Patched LLVM              : True\n",
      "SVML Operational                              : False\n",
      "\n",
      "__Threading Layer Information__\n",
      "TBB Threading Layer Available                 : False\n",
      "+--> Disabled due to Unknown import problem.\n",
      "OpenMP Threading Layer Available              : True\n",
      "+-->Vendor: GNU\n",
      "Workqueue Threading Layer Available           : True\n",
      "+-->Workqueue imported successfully.\n",
      "\n",
      "__Numba Environment Variable Information__\n",
      "None found.\n",
      "\n",
      "__Conda Information__\n",
      "Conda not available.\n",
      "\n",
      "__Installed Packages__\n",
      "Package                       Version\n",
      "----------------------------- -----------\n",
      "alabaster                     0.7.12\n",
      "archspec                      0.1.2\n",
      "argcomplete                   1.12.3\n",
      "argon2-cffi                   20.1.0\n",
      "asttokens                     2.0.5\n",
      "async-generator               1.10\n",
      "attrs                         20.3.0\n",
      "Babel                         2.9.1\n",
      "backcall                      0.2.0\n",
      "bcrypt                        3.2.0\n",
      "bleach                        3.3.0\n",
      "bokeh                         2.4.2\n",
      "build                         0.5.1\n",
      "certifi                       2020.12.5\n",
      "cffi                          1.14.5\n",
      "chardet                       4.0.0\n",
      "click                         7.1.2\n",
      "cloudpickle                   2.0.0\n",
      "cmake                         3.21.1\n",
      "cryptography                  36.0.2\n",
      "cupy-cuda114                  10.5.0\n",
      "cycler                        0.10.0\n",
      "Cython                        0.29.23\n",
      "dask                          2022.2.1\n",
      "decorator                     5.0.7\n",
      "defusedxml                    0.7.1\n",
      "distributed                   2022.2.1\n",
      "distro                        1.7.0\n",
      "docker                        5.0.3\n",
      "docker-compose                1.29.2\n",
      "dockerpty                     0.4.1\n",
      "docopt                        0.6.2\n",
      "docutils                      0.16\n",
      "easybuild                     4.4.0\n",
      "easybuild-easyblocks          4.4.0\n",
      "easybuild-easyconfigs         4.4.0\n",
      "easybuild-framework           4.4.0\n",
      "entrypoints                   0.3\n",
      "executing                     0.8.3\n",
      "Faker                         8.14.0\n",
      "fastrlock                     0.6\n",
      "flake8                        3.9.2\n",
      "fonttools                     4.33.3\n",
      "fsspec                        2022.2.0\n",
      "glumpy                        1.2.0\n",
      "HeapDict                      1.0.1\n",
      "hpccm                         21.8.0\n",
      "idna                          2.10\n",
      "imagesize                     1.2.0\n",
      "ipydatawidgets                4.2.0\n",
      "ipykernel                     5.5.3\n",
      "ipyparallel                   8.1.0\n",
      "ipython                       8.4.0\n",
      "ipython-genutils              0.2.0\n",
      "ipywidgets                    7.6.3\n",
      "jedi                          0.18.0\n",
      "Jinja2                        2.11.3\n",
      "jmespath                      0.10.0\n",
      "jsonschema                    3.2.0\n",
      "jupyter                       1.0.0\n",
      "jupyter-client                6.1.12\n",
      "jupyter-console               6.4.0\n",
      "jupyter-core                  4.7.1\n",
      "jupyterlab-pygments           0.1.2\n",
      "jupyterlab-widgets            1.0.0\n",
      "Kivy                          2.0.0\n",
      "Kivy-Garden                   0.1.4\n",
      "kiwisolver                    1.3.1\n",
      "llvmlite                      0.38.1\n",
      "locket                        0.2.1\n",
      "lxml                          4.6.3\n",
      "MarkupSafe                    1.1.1\n",
      "matplotlib                    3.5.2\n",
      "matplotlib-inline             0.1.2\n",
      "mccabe                        0.6.1\n",
      "memory-profiler               0.60.0\n",
      "mistune                       0.8.4\n",
      "mpmath                        1.2.1\n",
      "msgpack                       1.0.3\n",
      "my_primes                     0.0.0\n",
      "nbclient                      0.5.3\n",
      "nbconvert                     6.0.7\n",
      "nbformat                      5.1.3\n",
      "nest-asyncio                  1.5.1\n",
      "notebook                      6.3.0\n",
      "numba                         0.55.2\n",
      "numpy                         1.22.4\n",
      "packaging                     20.9\n",
      "pandas                        1.3.3\n",
      "pandocfilters                 1.4.3\n",
      "paramiko                      2.10.3\n",
      "parso                         0.8.2\n",
      "partd                         1.2.0\n",
      "pep517                        0.11.0\n",
      "pexpect                       4.8.0\n",
      "pickleshare                   0.7.5\n",
      "Pillow                        8.2.0\n",
      "pip                           22.0.4\n",
      "prometheus-client             0.10.1\n",
      "prompt-toolkit                3.0.18\n",
      "psutil                        5.8.0\n",
      "ptyprocess                    0.7.0\n",
      "pure-eval                     0.2.2\n",
      "pycodestyle                   2.7.0\n",
      "pycparser                     2.20\n",
      "pyflakes                      2.3.1\n",
      "pygame                        2.0.1\n",
      "Pygments                      2.8.1\n",
      "PyNaCl                        1.5.0\n",
      "PyOpenGL                      3.1.5\n",
      "PyOpenGL-accelerate           3.1.5\n",
      "pyparsing                     2.4.7\n",
      "pyrsistent                    0.17.3\n",
      "python-dateutil               2.8.1\n",
      "python-dotenv                 0.19.2\n",
      "pythreejs                     2.3.0\n",
      "pytz                          2021.1\n",
      "PyYAML                        5.4.1\n",
      "pyzmq                         22.0.3\n",
      "qtconsole                     5.0.3\n",
      "QtPy                          1.9.0\n",
      "requests                      2.25.1\n",
      "s-tui                         1.1.3\n",
      "scipy                         1.8.1\n",
      "semver                        2.13.0\n",
      "Send2Trash                    1.5.0\n",
      "setuptools                    57.4.0\n",
      "Shapely                       1.8.1.post1\n",
      "six                           1.15.0\n",
      "snakeviz                      2.1.0\n",
      "snowballstemmer               2.1.0\n",
      "sortedcontainers              2.4.0\n",
      "Sphinx                        3.5.4\n",
      "sphinx-rtd-theme              0.5.2\n",
      "sphinxcontrib-applehelp       1.0.2\n",
      "sphinxcontrib-devhelp         1.0.2\n",
      "sphinxcontrib-htmlhelp        2.0.0\n",
      "sphinxcontrib-jsmath          1.0.1\n",
      "sphinxcontrib-qthelp          1.0.3\n",
      "sphinxcontrib-serializinghtml 1.1.5\n",
      "stack-data                    0.2.0\n",
      "sympy                         1.8\n",
      "tbb                           2021.2.0\n",
      "tblib                         1.7.0\n",
      "terminado                     0.9.4\n",
      "testpath                      0.4.4\n",
      "text-unidecode                1.3\n",
      "texttable                     1.6.4\n",
      "toml                          0.10.2\n",
      "tomli                         1.0.4\n",
      "toolz                         0.11.2\n",
      "tornado                       6.1\n",
      "tqdm                          4.62.3\n",
      "traitlets                     5.0.5\n",
      "traittypes                    0.2.1\n",
      "triangle                      20200424\n",
      "typing_extensions             4.1.1\n",
      "urllib3                       1.25.11\n",
      "urwid                         2.1.2\n",
      "urwid-readline                0.13\n",
      "versioneer                    0.18\n",
      "wcwidth                       0.2.5\n",
      "webencodings                  0.5.1\n",
      "websocket-client              0.59.0\n",
      "wheel                         0.37.1\n",
      "widgetsnbextension            3.5.1\n",
      "zict                          2.1.0\n",
      "\n",
      "No errors reported.\n",
      "\n",
      "\n",
      "__Warning log__\n",
      "Warning: Conda not available.\n",
      " Error was [Errno 2] No such file or directory: 'conda'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "If requested, please copy and paste the information between\n",
      "the dashed (----) lines, or from a given specific section as\n",
      "appropriate.\n",
      "\n",
      "=============================================================\n",
      "IMPORTANT: Please ensure that you are happy with sharing the\n",
      "contents of the information present, any information that you\n",
      "wish to keep private you should remove before sharing.\n",
      "=============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!numba -s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this command does not work out of the box for your Numba installation, you may have to set the `CUDA_HOME` environment variable to point to your CUDA installation.\n",
    "\n",
    "## Writing the first kernel\n",
    "\n",
    "Here is a vector addition kernel that takes two vectors as input and writes the sum in a third vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba.cuda as cuda\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def _vecadd_cuda(z, x, y):\n",
    "    i = cuda.grid(1)\n",
    "    N = x.shape[0]\n",
    "    if i >= N:\n",
    "        return\n",
    "\n",
    "    z[i] = x[i] + y[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty simple, right? Let's explain each line of this code.\n",
    "\n",
    "The `@cuda.jit` decorator will compile the following function into a CUDA kernel at runtime. We will see the cost of it later on.\n",
    "\n",
    "A CUDA kernel in Numba is a Python function that does *not* return a value. This is in accordance with CUDA, where kernel functions are declared `void`. The arguments of the function can be either Numpy arrays or scalars of a Numba recognized type.\n",
    "\n",
    "CUDA kernels specify the work to be done by a single GPU thread. Due to the massive hardware parallelism available on the GPU, a single GPU thread gets only a tiny portion of work, in this case, the sumation of a single element in the vectors. Since each element is independent from each other, we can safely create as many threads as the elements of the target vectors and spawn them (we will see how later).\n",
    "\n",
    "Threads on the GPU as organized in groups, called CUDA blocks. There are constraints on the maximum number of threads a block can contain. For the P100 GPUs on Daint, this is 1024. This means that you will need multiple blocks to calculate the sum of large vectors. The blocks that comprise a kernel form the *grid*. Threads inside a block are numbered sequentially and the blocks inside the grid are numbered, too. The figure below shows the arrangement of threads for the vector addition example.\n",
    "\n",
    "![Arrangement of threads in CUDA blocks](figs/cuda-blocks.png)\n",
    "\n",
    "In order to obtain the i-th element of the vector given a block size $B$, you would have to calculate the following:\n",
    "\n",
    "\\begin{equation}\n",
    "i = i_{b}B + i_{t}\n",
    "\\end{equation}\n",
    "\n",
    "where $i_{b}$ is the block index and $i_{t}$ is the thread index. CUDA provides this information and Numba makes it available, so that the above statement would be written as follows:\n",
    "\n",
    "```python\n",
    "i = cuda.blockIdx.x*cuda.blockDim.x + threadIdx.x\n",
    "```\n",
    "\n",
    "Blocks and grids can be three dimensional, thus the `.x` attribute in all of these variables (the other dimensions are accessed through the `.y` and `.z` attributes).\n",
    "\n",
    "Since obtaining the absolute position of a thread in a CUDA is quite common operation, Numba offers the convenience function `grid()`, which essentially does automatically the above calculation:\n",
    "\n",
    "```python\n",
    "i = cuda.grid(1)\n",
    "```\n",
    "\n",
    "The argument passed to the `grid()` function is the number of dimensions of the grid.\n",
    "\n",
    "The next three statements in the code simply check that we don't overrun the arrays in case that their dimension is not a multiple of the block size. In this case, some threads of the last block will remain idle:\n",
    "\n",
    "```python\n",
    "    N = x.shape[0]\n",
    "    if i >= N:\n",
    "        return\n",
    "```\n",
    "\n",
    "> The threads inside a block are run in batches of 32 at once, called *warps*. All the threads of the warp execute the same instruction. If the program control flow diverges for some of the threads of the warp, e.g., due to an `if` condition, the branches will be executed sequentially by disabling the non participating threads. This condition is called *warp divergence* and may incur a performance penalty.\n",
    "\n",
    "Finally, the `z[i] = x[i] + y[i]` computes the actual sum.\n",
    "\n",
    "\n",
    "## Preparing the data for the GPU\n",
    "\n",
    "All Numba CUDA kernels operate on data residing on the GPU. That means that the `x`, `y` and `z` arrays must be  transferred from the host (CPU) to the device (accelerator) before calling the CUDA kernel.\n",
    "\n",
    "Let's create first two vectors on the host:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "N = 1024\n",
    "x = rng.random(N)\n",
    "y = rng.random(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute our reference result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_ref = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to transfer the vectors to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `d_x` and `d_y` are Numpy *array-like* objects that have their data mapped in the GPU memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numba.cuda.cudadrv.devicearray.DeviceNDArray at 0x7f47956b13a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `z` array, we don't need to create it on the host and copy it, since it is essentially an output only array. We can simply allocate it directly on the GPU and copy it out when the kernel finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_z = cuda.device_array_like(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will simply allocate an array like `x`, i.e., with the same element type, shape and order, on the GPU.\n",
    "\n",
    "Data is ready, now it's time to call the kernel!\n",
    "\n",
    "## Invoking a CUDA kernel\n",
    "\n",
    "When invoking a CUDA kernel you have to specify the block size to use and the corresponding grid size. Picking the right size of block is not always straightforward, but usually values between 64 and 256 are good enough. \n",
    "\n",
    "> The block size has a direct effect on the *occupancy* of each GPU SM, i.e., how much the actual hardware threads of the SM are utilized, and as a result to the occupancy of the whole GPU. Performance-wise, though, it does not have such a big impact. Nvidia provides a nice tool for calculating the occupancy of the GPU, that you can find [here](https://docs.nvidia.com/cuda/cuda-occupancy-calculator/CUDA_Occupancy_Calculator.xls).\n",
    "\n",
    "For our kernel, we will select a block size of 128 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having decided the block size we need to set up the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks = N // block_size\n",
    "if N % block_size:\n",
    "    num_blocks += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to call the kernel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teojgo/.local/lib/python3.9/site-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: Grid size (8) < 2 * SM count (26) will likely result in GPU under utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "_vecadd_cuda[num_blocks, block_size](d_z, d_x, d_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the block and the grid could have been two-dimensional (not in this example), in which case you would just define them as tuples.\n",
    "\n",
    "## Copying back the results\n",
    "\n",
    "As mentioned before, kernels operate on GPU data only. We need a way to transfer back to the host the result, which is the `d_z` array. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = d_z.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's validate the result though to make sure that everything has worked fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(z_ref, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "For completeness and easy reference, here is the whole vector addition example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teojgo/.local/lib/python3.9/site-packages/numba/cuda/compiler.py:726: NumbaPerformanceWarning: Grid size (8) < 2 * SM count (26) will likely result in GPU under utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "import numba.cuda as cuda\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def _vecadd_cuda(z, x, y):\n",
    "    '''The CUDA kernel'''\n",
    "    i = cuda.grid(1)\n",
    "    N = x.shape[0]\n",
    "    if i >= N:\n",
    "        return\n",
    "\n",
    "    z[i] = x[i] + y[i]\n",
    "\n",
    "\n",
    "# Set up the host vectors\n",
    "N = 1000\n",
    "x = rng.random(N)\n",
    "y = rng.random(N)\n",
    "\n",
    "# Copy and allocate data on the device\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "d_z = cuda.device_array_like(x)\n",
    "\n",
    "# Set up the kernel invocation\n",
    "block_size = 128\n",
    "num_blocks = N // block_size\n",
    "if N % block_size:\n",
    "    num_blocks += 1\n",
    "\n",
    "# Call the kernel\n",
    "_vecadd_cuda[num_blocks, block_size](d_z, d_x, d_y)\n",
    "\n",
    "# Copy back the result to the host\n",
    "res = d_z.copy_to_host()\n",
    "\n",
    "# Validate the result\n",
    "assert np.allclose(x + y, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "> 1. Make the array sufficiently large and time the Numpy version of the sum `x + y`.\n",
    "> 2. Now time the call to the CUDA kernel with `%timeit -n1 -r1`. What do you see?\n",
    "> 3. Try timing the CUDA kernels with `%timeit -n1 -r2`. What is happening?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
